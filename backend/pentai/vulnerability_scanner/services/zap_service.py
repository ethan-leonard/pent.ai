import logging
import time
from zapv2 import ZAPv2
from vulnerability_scanner.models import Vulnerability

logger = logging.getLogger(__name__)

class ZAPService:
    """Service for verifying connection to OWASP ZAP and performing scans"""
    
    def __init__(self):
        """Initialize the ZAP service with connection to ZAP API"""
        self.apiKey = None
        # Use localhost since startup.sh maps ZAP to port 8090 on localhost
        self.zap = ZAPv2(
            apikey=self.apiKey, 
            proxies={'http': 'http://localhost:8090', 'https': 'http://localhost:8090'},
        )
        
    def verify_connection(self, url):
        """
        Verify that we can connect to ZAP and access a URL.
        (Existing connection test)
        """
        try:
            logger.info("Attempting to connect to ZAP API")
            version = self.zap.core.version
            logger.info(f"Connected to ZAP version: {version}")
            return {
                "zap_connected": True,
                "zap_version": version,
                "success": True,
                "message": f"Connected to ZAP (version {version})"
            }
        except Exception as e:
            logger.error(f"Error connecting to ZAP: {str(e)}")
            return {
                "zap_connected": False,
                "success": False,
                "error": str(e),
                "message": f"Failed to connect to ZAP: {str(e)}"
            }
    
    def traditional_spider_scan(self, url):
        """
        Perform a traditional spider scan on the target URL and return results.
        """
        try:
            logger.info(f"Opening target URL: {url}")
            self.zap.urlopen(url)
            time.sleep(2)
            
            # Start traditional spider scan
            logger.info(f"Starting traditional spider scan on {url}")
            spider_id = self.zap.spider.scan(url)
            while True:
                status_str = self.zap.spider.status(spider_id)
                try:
                    percent = int(status_str)
                except ValueError:
                    logger.error(f"Unexpected spider status: {status_str}")
                    break
                logger.info(f"Traditional spider scan progress: {percent}%")
                if percent >= 100:
                    break
                time.sleep(1)
            
            # Get traditional spider results
            try:
                spider_results = self.zap.spider.results(spider_id)
                logger.info(f"Traditional spider scan completed. Found {len(spider_results)} URLs.")
            except Exception as e:
                logger.error(f"Error getting spider results: {str(e)}")
                spider_results = []
            
            return {
                "success": True,
                "message": "Traditional spider scan completed.",
                "spider_results": spider_results
            }
        except Exception as e:
            logger.error(f"Error during traditional spider scan: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "message": f"Traditional spider scan failed: {str(e)}"
            }
    
    def ajax_spider_scan(self, url):
        """
        Perform an Ajax spider scan on the target URL and return results.
        """
        try:
            logger.info(f"Opening target URL for Ajax spider: {url}")
            self.zap.urlopen(url)
            time.sleep(2)
            
            logger.info(f"Starting Ajax spider scan on {url}")
            # Ensure ZAP is responsive before starting Ajax spider
            self.zap.core.version
            
            # Start the Ajax spider
            self.zap.ajaxSpider.scan(url)
            
            # Set timeout for Ajax spider (5 minutes)
            ajax_timeout = time.time() + 300
            
            # Monitor Ajax spider status
            while self.zap.ajaxSpider.status == 'running':
                if time.time() > ajax_timeout:
                    logger.warning("Ajax spider timed out after 5 minutes, stopping.")
                    try:
                        self.zap.ajaxSpider.stop()
                    except Exception as e:
                        logger.error(f"Error stopping Ajax spider: {str(e)}")
                    break
                logger.info(f"Ajax spider status: {self.zap.ajaxSpider.status}")
                time.sleep(5)
            
            # Make sure to wait a moment for the Ajax spider to fully finish
            time.sleep(2)
            
            # Get Ajax spider results (all of them) - with error handling
            try:
                # Verify ZAP is still responsive
                self.zap.core.version
                
                ajax_results = self.zap.ajaxSpider.results(start=0, count=1000)
                ajax_urls = [item.get('url', '') for item in ajax_results if isinstance(item, dict) and 'url' in item]
                logger.info(f"Ajax spider completed. Found {len(ajax_urls)} URLs.")
            except Exception as e:
                logger.error(f"Error retrieving Ajax spider results: {str(e)}")
                ajax_urls = []
                
            # Make sure ZAP is still responsive after Ajax spider
            try:
                self.zap.core.version
                logger.info("ZAP API is still responsive after Ajax Spider")
            except Exception as e:
                logger.error(f"ZAP API became unresponsive: {str(e)}")
                # Try to reconnect
                try:
                    self.zap = ZAPv2(
                        apikey=self.apiKey, 
                        proxies={'http': 'http://localhost:8090', 'https': 'http://localhost:8090'},
                    )
                    logger.info("Successfully reconnected to ZAP API")
                except Exception as re:
                    logger.error(f"Failed to reconnect to ZAP: {str(re)}")
            
            return {
                "success": True,
                "message": "Ajax spider scan completed.",
                "spider_results": ajax_urls
            }
        except Exception as e:
            logger.error(f"Error during Ajax spider scan: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "message": f"Ajax spider scan failed: {str(e)}"
            }

    def active_scan_all(self, url):
        try:
            logger.info(f"Starting active scan on {url}")
            active_scan_id = self.zap.ascan.scan(url, recurse=True)
            start_time = time.time()
            timeout = 500  # seconds
            
            while True:
                status_str = self.zap.ascan.status(active_scan_id)
                if status_str == "does_not_exist":
                    logger.warning("Active scan status returned 'does_not_exist', treating as complete.")
                    break
                try:
                    status_int = int(status_str)
                except ValueError:
                    logger.error(f"Unexpected active scan status: {status_str}")
                    return {
                        "success": False,
                        "error": f"Unexpected scan status: {status_str}",
                        "message": f"Active scan failed with unexpected status: {status_str}"
                    }
                logger.info(f"Active scan progress: {status_int}%")
                if status_int >= 100:
                    break
                if time.time() - start_time > timeout:
                    logger.error("Active scan timed out")
                    return {
                        "success": False,
                        "error": "Timeout",
                        "message": f"Active scan timed out after {timeout} seconds"
                    }
                time.sleep(5)
            
            logger.info("Active scan completed.")
            time.sleep(2)
            
            # Retrieve all alerts for the target URL
            all_alerts = self.zap.alert.alerts(baseurl=url)
            logger.info(f"Total alerts found: {len(all_alerts)}")
            
            # Define the vulnerability types we want to include
            target_vuln_types = [
                "SQL Injection",
                "Content Security Policy",
                "Cross-Origin Resource Sharing",  # Will match CORS vulnerabilities
                "Cross Domain JavaScript Source"   # Will match Cross-Domain JavaScript Source Inclusion
            ]
            
            # Filter alerts to only include the specified vulnerability types
            filtered_alerts = []
            for alert in all_alerts:
                alert_name = alert.get('alert', '')
                for target_type in target_vuln_types:
                    if target_type.lower() in alert_name.lower():
                        filtered_alerts.append(alert)
                        break
            
            logger.info(f"Filtered to {len(filtered_alerts)} alerts of specified types")
            
            # Create a risk level mapping for sorting
            risk_priority = {
                'high': 0,
                'medium': 1,
                'low': 2,
                'informational': 3,
                'info': 3,  # Sometimes 'info' is used instead of 'informational'
                'unknown': 4
            }
            
            # Sort by risk level (high first) and then alphabetically
            sorted_alerts = sorted(
                filtered_alerts,
                key=lambda a: (
                    risk_priority.get(a.get('risk', '').lower(), 5),  # Default to lowest priority if not in mapping
                    a.get('alert', '').lower()  # Then sort alphabetically by alert name
                )
            )
            
            # Remove duplicates (by alert name and URL)
            unique_alerts = []
            seen = set()
            for alert in sorted_alerts:
                # Create a unique key based on alert name and URL
                key = (alert.get('alert', ''), alert.get('url', ''))
                if key not in seen:
                    seen.add(key)
                    unique_alerts.append(alert)
            
            # Limit to 10 results
            final_alerts = unique_alerts[:10]
            logger.info(f"Selected {len(final_alerts)} vulnerabilities after filtering and sorting")
            
            # Save vulnerabilities to the database
            for alert in final_alerts:
                Vulnerability.objects.create(
                    name=alert.get('alert', 'Unknown'),
                    description=alert.get('attack', '') or alert.get('description', ''),
                    url=alert.get('url', url),
                    risk=alert.get('risk', 'Unknown'),
                    confidence=alert.get('confidence', 'Unknown'),
                    alert_id=alert.get('alertId', '')
                )
                
            return {
                "success": True,
                "message": f"Active scan completed. Found {len(final_alerts)} significant vulnerabilities",
                "alerts": final_alerts
            }
        except Exception as e:
            logger.error(f"Error during active scan: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "message": f"Active scan failed: {str(e)}"
            }